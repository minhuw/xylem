//! Basic Redis benchmarking example
//!
//! This example demonstrates how to use Xylem programmatically to benchmark Redis.
//! It shows:
//! - Setting up a Redis protocol worker
//! - Running a single-threaded benchmark
//! - Collecting and displaying latency statistics
//!
//! ## Prerequisites
//!
//! - Redis server running on localhost:6379
//!   Start with Docker: `docker run -d -p 6379:6379 redis:latest`
//!   Or use: `docker compose -f tests/redis/docker-compose.yml up -d`
//!
//! ## Running
//!
//! ```bash
//! # Make sure Redis is running first
//! docker run -d -p 6379:6379 redis:latest
//!
//! # Run the example
//! cargo run --example redis_basic
//! ```

use std::time::Duration;
use xylem_core::stats::{GroupStatsCollector, SamplingPolicy};
use xylem_core::threading::{Worker, WorkerConfig};
use xylem_transport::TcpTransportFactory;

// Protocol adapter to bridge xylem_protocols::Protocol with worker Protocol trait
struct ProtocolAdapter<P: xylem_protocols::Protocol> {
    inner: P,
}

impl<P: xylem_protocols::Protocol> ProtocolAdapter<P> {
    fn new(protocol: P) -> Self {
        Self { inner: protocol }
    }
}

impl<P: xylem_protocols::Protocol> xylem_core::threading::worker::Protocol for ProtocolAdapter<P> {
    type RequestId = P::RequestId;

    fn next_request(
        &mut self,
        conn_id: usize,
    ) -> xylem_core::threading::worker::Request<Self::RequestId> {
        let request = self.inner.next_request(conn_id);
        xylem_core::threading::worker::Request::new(
            request.data,
            request.request_id,
            xylem_core::threading::worker::RequestMeta { is_warmup: request.metadata.is_warmup },
        )
    }

    fn regenerate_request(
        &mut self,
        conn_id: usize,
        original_request_id: Self::RequestId,
    ) -> xylem_core::threading::worker::Request<Self::RequestId> {
        let request = self.inner.regenerate_request(conn_id, original_request_id);
        xylem_core::threading::worker::Request::new(
            request.data,
            request.request_id,
            xylem_core::threading::worker::RequestMeta { is_warmup: request.metadata.is_warmup },
        )
    }

    fn parse_response(
        &mut self,
        conn_id: usize,
        data: &[u8],
    ) -> anyhow::Result<(usize, Option<Self::RequestId>)> {
        self.inner.parse_response(conn_id, data)
    }

    fn name(&self) -> &'static str {
        self.inner.name()
    }

    fn reset(&mut self) {
        self.inner.reset()
    }
}

fn main() -> anyhow::Result<()> {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘       Xylem Redis Benchmark Example           â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // Check if Redis is available
    println!("ðŸ”Œ Checking Redis connection...");
    let target_addr = "127.0.0.1:6379".parse()?;

    match std::net::TcpStream::connect_timeout(&target_addr, Duration::from_secs(1)) {
        Ok(_) => println!("âœ“ Redis is available on 127.0.0.1:6379\n"),
        Err(_) => {
            eprintln!("âŒ Error: Redis is not running on 127.0.0.1:6379");
            eprintln!("\nPlease start Redis with Docker:");
            eprintln!("  docker run -d -p 6379:6379 redis:latest");
            eprintln!("\nOr use the just command:");
            eprintln!("  just redis-start");
            std::process::exit(1);
        }
    }

    // Configuration
    let duration = Duration::from_secs(5);
    let value_size = 64;
    let conn_count = 1;
    let max_pending_per_conn = 1;

    println!("ðŸ“Š Benchmark Configuration:");
    println!("  â€¢ Target: 127.0.0.1:6379");
    println!("  â€¢ Protocol: Redis GET");
    println!("  â€¢ Duration: {} seconds", duration.as_secs());
    println!("  â€¢ Value size: {} bytes", value_size);
    println!("  â€¢ Connections: {}", conn_count);
    println!("  â€¢ Max pending per connection: {}", max_pending_per_conn);
    println!("  â€¢ Mode: Closed-loop (max throughput)\n");

    // Create Redis protocol with GET operations and embedded workload
    let selector =
        Box::new(xylem_protocols::FixedCommandSelector::new(xylem_protocols::redis::RedisOp::Get));
    let key_gen = xylem_protocols::workload::KeyGeneration::sequential(0);
    let protocol =
        xylem_protocols::redis::RedisProtocol::with_workload(selector, key_gen, value_size);
    let protocol = ProtocolAdapter::new(protocol);

    // Create stats collector with unlimited sampling (collect all latencies)
    let mut stats = GroupStatsCollector::new();
    stats.register_group(0, &SamplingPolicy::Unlimited);

    // Create worker configuration
    let worker_config = WorkerConfig {
        target: target_addr,
        duration,
        conn_count,
        max_pending_per_conn,
    };

    // Create and configure the worker
    let mut worker =
        Worker::with_closed_loop(&TcpTransportFactory::default(), protocol, stats, worker_config)?;

    // Run the benchmark
    println!("ðŸš€ Starting benchmark...");
    let start = std::time::Instant::now();

    worker.run()?;

    let elapsed = start.elapsed();

    // Extract and display results
    let stats = worker.into_stats();
    let global_stats = stats.global();
    let basic_stats = global_stats.calculate_basic_stats();

    // Calculate aggregated stats with percentiles (95% confidence level)
    let agg_stats = xylem_core::stats::aggregate_stats(global_stats, elapsed, 0.95);

    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘              Benchmark Results                 â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // Throughput metrics
    println!("ðŸ“ˆ Throughput:");
    println!("  â€¢ Total requests: {}", agg_stats.total_requests);
    println!("  â€¢ Total responses: {}", global_stats.rx_requests());
    println!("  â€¢ Requests/sec: {:.2}", agg_stats.throughput_rps);
    println!("  â€¢ Throughput: {:.2} MB/s", agg_stats.throughput_mbps);

    // Latency metrics
    println!("\nâ±ï¸  Latency (microseconds):");
    println!("  â€¢ Min:    {:>10.2} Î¼s", basic_stats.min.as_micros());
    println!("  â€¢ Mean:   {:>10.2} Î¼s", agg_stats.mean_latency.as_micros());
    println!("  â€¢ Median: {:>10.2} Î¼s", agg_stats.latency_p50.as_micros());
    println!("  â€¢ p95:    {:>10.2} Î¼s", agg_stats.latency_p95.as_micros());
    println!("  â€¢ p99:    {:>10.2} Î¼s", agg_stats.latency_p99.as_micros());
    println!("  â€¢ p99.9:  {:>10.2} Î¼s", agg_stats.latency_p999.as_micros());
    println!("  â€¢ Max:    {:>10.2} Î¼s", basic_stats.max.as_micros());
    println!("  â€¢ Std Dev: {:>9.2} Î¼s", agg_stats.std_dev.as_micros());

    println!("\nâœ… Benchmark completed successfully!\n");

    // Display some helpful tips
    println!("ðŸ’¡ Next steps:");
    println!("  â€¢ Try increasing connections: Change conn_count to 4 or 8");
    println!("  â€¢ Test pipelining: Set max_pending_per_conn to 16");
    println!("  â€¢ Test rate limiting: Use RateControl::Fixed {{ rate: 1000.0 }}");
    println!("  â€¢ Run multi-threaded: Check out the multi-threaded examples");

    Ok(())
}
